{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxfDnfhDMvDxdwtWhlMI6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RounabhSahu/ML_Proxxmed/blob/main/U_Net_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvGHeeWNqxoU",
        "outputId": "79e2fb79-c76a-48de-da6b-e794a333ad2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import random\n",
        "import nibabel\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from nibabel import load\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import Sequence\n",
        "from IPython.display import Image, display\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual,FloatRangeSlider,FloatSlider\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from google.colab import drive\n",
        "from scipy.ndimage import zoom\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your data path\n",
        "data_path = '/content/drive/MyDrive/HYPODENSITY-DATA/'"
      ],
      "metadata": {
        "id": "65Duh8b6qy1a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainHypoDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_filenames, mask_filenames, batch_size, image_size):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.mask_filenames = mask_filenames\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.length = int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.mask_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        x = np.zeros((self.batch_size, *self.image_size, 1))\n",
        "        y = np.zeros((self.batch_size, *self.image_size, 1))\n",
        "\n",
        "        for i, (image_filename, mask_filename) in enumerate(zip(batch_x, batch_y)):\n",
        "            image = nib.load(image_filename)\n",
        "            mask = nib.load(mask_filename)\n",
        "            # get the data from the image object\n",
        "            image_data = image.get_fdata()\n",
        "            mask_data = mask.get_fdata()\n",
        "            # get random slice from the volumes\n",
        "            slice_index = np.random.randint(0, image_data.shape[2] - 1)\n",
        "            x[i, :, :, 0] = image_data[:, :, slice_index]\n",
        "            y[i, :, :, 0] = (mask_data[:, :, slice_index] > 0).astype(np.float32)  # Assuming mask is binary\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Get the list of train images and masks\n",
        "train_images = sorted(glob(os.path.join(data_path, '*', '*_NCCT.nii.gz')))\n",
        "train_masks = sorted(glob(os.path.join(data_path, '*', '*_ROI.nii.gz')))\n",
        "\n",
        "# Set batch size and image size\n",
        "batch_size = 1\n",
        "image_size = (512, 512)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = BrainHypoDataGenerator(train_images[:10], train_masks[:10], batch_size, image_size)\n",
        "val_generator = BrainHypoDataGenerator(train_images[10:], train_masks[10:], batch_size, image_size)"
      ],
      "metadata": {
        "id": "4gAPKl_gq17w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def encoder(inputs, filters, pool_size):\n",
        "    conv_pool = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv_pool = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(conv_pool)\n",
        "    return conv_pool\n",
        "\n",
        "def decoder(inputs, concat_input, filters, transpose_size):\n",
        "    up = tf.keras.layers.Concatenate()([tf.keras.layers.Conv2DTranspose(filters, transpose_size, strides=(2, 2), padding='same')(inputs), concat_input])\n",
        "    up = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(up)\n",
        "    return up\n",
        "\n",
        "def UNet(img_size=(512, 512, 1)):\n",
        "    inputs = tf.keras.Input(img_size)\n",
        "    print(inputs.shape)\n",
        "    print()\n",
        "\n",
        "    # Encoder\n",
        "    conv_pool1 = encoder(inputs, 32, (2, 2))\n",
        "    print(\"\\t Enc. 1 ->\", conv_pool1.shape)\n",
        "    print()\n",
        "    conv_pool2 = encoder(conv_pool1, 64, (2, 2))\n",
        "    print(\"\\t\\t Enc. 2 ->\", conv_pool2.shape)\n",
        "    print()\n",
        "    conv_pool3 = encoder(conv_pool2, 128, (2, 2))\n",
        "    print(\"\\t\\t\\t Enc. 3 ->\", conv_pool3.shape)\n",
        "    print()\n",
        "    conv_pool4 = encoder(conv_pool3, 256, (2, 2))\n",
        "    print(\"\\t\\t\\t\\t Enc. 4 ->\", conv_pool4.shape)\n",
        "    print()\n",
        "\n",
        "    # Bottleneck\n",
        "    bridge = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv_pool4)\n",
        "    print(\"\\t\\t\\t\\t\\t Bridge Conv ->\", bridge.shape)\n",
        "    print()\n",
        "\n",
        "    # Decoder\n",
        "    up6 = decoder(bridge, conv_pool3, 256, (2, 2))\n",
        "    print(\"\\t\\t\\t\\t Dec. 4 ->\", up6.shape)\n",
        "    print()\n",
        "    up7 = decoder(up6, conv_pool2, 128, (2, 2))\n",
        "    print(\"\\t\\t\\t Dec. 3 ->\", up7.shape)\n",
        "    print()\n",
        "    up8 = decoder(up7, conv_pool1, 64, (2, 2))\n",
        "    print(\"\\t\\t Dec. 2 ->\", up8.shape)\n",
        "    print()\n",
        "    up9 = decoder(up8, inputs, 32, (2, 2))\n",
        "    print(\"\\t Dec. 1 ->\", up9.shape)\n",
        "    print()\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(up9)\n",
        "    print(outputs.shape)\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# Set your input shape\n",
        "input_shape = (512, 512, 1)\n",
        "\n",
        "# Create the UNet model\n",
        "model = UNet(img_size=input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOWN2IourB6m",
        "outputId": "08c70a56-e3b7-4b1f-9064-61db6c7bc44a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 512, 512, 1)\n",
            "\n",
            "\t Enc. 1 -> (None, 256, 256, 32)\n",
            "\n",
            "\t\t Enc. 2 -> (None, 128, 128, 64)\n",
            "\n",
            "\t\t\t Enc. 3 -> (None, 64, 64, 128)\n",
            "\n",
            "\t\t\t\t Enc. 4 -> (None, 32, 32, 256)\n",
            "\n",
            "\t\t\t\t\t Bridge Conv -> (None, 32, 32, 512)\n",
            "\n",
            "\t\t\t\t Dec. 4 -> (None, 64, 64, 256)\n",
            "\n",
            "\t\t\t Dec. 3 -> (None, 128, 128, 128)\n",
            "\n",
            "\t\t Dec. 2 -> (None, 256, 256, 64)\n",
            "\n",
            "\t Dec. 1 -> (None, 512, 512, 32)\n",
            "\n",
            "(None, 512, 512, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1.):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "class BrainHypoDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_filenames, mask_filenames, batch_size, image_size):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.mask_filenames = mask_filenames\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "      batch_y = self.mask_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "      x = np.zeros((self.batch_size, *self.image_size))\n",
        "      y = np.zeros((self.batch_size, *self.image_size, 1))\n",
        "\n",
        "      for i, (image_filename, mask_filename) in enumerate(zip(batch_x, batch_y)):\n",
        "          image = nib.load(image_filename)\n",
        "          mask = nib.load(mask_filename)\n",
        "          # get the data from the image object\n",
        "          image_data = image.get_fdata()\n",
        "          mask_data = mask.get_fdata()\n",
        "          # get random slice from the volumes\n",
        "          slice_index = np.random.randint(0, image_data.shape[2] - 1)\n",
        "          # Add channel dimension to image_data\n",
        "          x[i, :, :, 0] = image_data[:, :, slice_index]\n",
        "          # Add channel dimension to mask_data\n",
        "          y[i, :, :, 0] = np.expand_dims((mask_data[:, :, slice_index] > 0).astype(np.float32), axis=-1)\n",
        "\n",
        "      return x, y\n",
        "\n",
        "# Get the list of train, validation, and test images and masks\n",
        "train_images = sorted(glob(os.path.join(data_path, '*', '*_NCCT.nii.gz')))\n",
        "train_masks = sorted(glob(os.path.join(data_path, '*', '*_ROI.nii.gz')))\n",
        "val_images = sorted(glob(os.path.join(data_path, '*', '*_NCCT.nii.gz')))\n",
        "val_masks = sorted(glob(os.path.join(data_path, '*', '*_ROI.nii.gz')))\n",
        "test_images = sorted(glob(os.path.join(data_path, '*', '*_NCCT.nii.gz')))\n",
        "test_masks = sorted(glob(os.path.join(data_path, '*', '*_ROI.nii.gz')))\n",
        "\n",
        "# Set batch size and image size\n",
        "batch_size = 1\n",
        "image_size = (512, 512, 1)  # Adjusted to 3D input\n",
        "\n",
        "# Create data generators\n",
        "train_generator = BrainHypoDataGenerator(train_images, train_masks, batch_size, image_size)\n",
        "val_generator = BrainHypoDataGenerator(val_images, val_masks, batch_size, image_size)\n",
        "test_generator = BrainHypoDataGenerator(test_images, test_masks, batch_size, image_size)\n",
        "\n",
        "# Build the model\n",
        "def UNet(img_size=(512, 512, 1)):\n",
        "    inputs = tf.keras.Input(img_size)\n",
        "    print(inputs.shape)\n",
        "    print()\n",
        "\n",
        "    # Encoder\n",
        "    conv_pool1 = encoder(inputs, 32, (2, 2))\n",
        "    print(\"\\t Enc. 1 ->\", conv_pool1.shape)\n",
        "    print()\n",
        "    conv_pool2 = encoder(conv_pool1, 64, (2, 2))\n",
        "    print(\"\\t\\t Enc. 2 ->\", conv_pool2.shape)\n",
        "    print()\n",
        "    conv_pool3 = encoder(conv_pool2, 128, (2, 2))\n",
        "    print(\"\\t\\t\\t Enc. 3 ->\", conv_pool3.shape)\n",
        "    print()\n",
        "    conv_pool4 = encoder(conv_pool3, 256, (2, 2))\n",
        "    print(\"\\t\\t\\t\\t Enc. 4 ->\", conv_pool4.shape)\n",
        "    print()\n",
        "\n",
        "    # Bottleneck\n",
        "    bridge = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv_pool4)\n",
        "    print(\"\\t\\t\\t\\t\\t Bridge Conv ->\", bridge.shape)\n",
        "    print()\n",
        "\n",
        "    # Decoder\n",
        "    up6 = decoder(bridge, conv_pool3, 256, (2, 2))\n",
        "    print(\"\\t\\t\\t\\t Dec. 4 ->\", up6.shape)\n",
        "    print()\n",
        "    up7 = decoder(up6, conv_pool2, 128, (2, 2))\n",
        "    print(\"\\t\\t\\t Dec. 3 ->\", up7.shape)\n",
        "    print()\n",
        "    up8 = decoder(up7, conv_pool1, 64, (2, 2))\n",
        "    print(\"\\t\\t Dec. 2 ->\", up8.shape)\n",
        "    print()\n",
        "    up9 = decoder(up8, inputs, 32, (2, 2))\n",
        "    print(\"\\t Dec. 1 ->\", up9.shape)\n",
        "    print()\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(up9)\n",
        "    print(outputs.shape)\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "# Create UNet model\n",
        "model = UNet(img_size=image_size)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=[dice_coef])\n",
        "\n",
        "# Set up model checkpoint\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(train_generator),\n",
        "                    epochs=100,\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps=len(val_generator),\n",
        "                    callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yXG3YtTrJRn",
        "outputId": "7308422e-045a-4306-a817-390eda804c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 512, 512, 1)\n",
            "\n",
            "\t Enc. 1 -> (None, 256, 256, 32)\n",
            "\n",
            "\t\t Enc. 2 -> (None, 128, 128, 64)\n",
            "\n",
            "\t\t\t Enc. 3 -> (None, 64, 64, 128)\n",
            "\n",
            "\t\t\t\t Enc. 4 -> (None, 32, 32, 256)\n",
            "\n",
            "\t\t\t\t\t Bridge Conv -> (None, 32, 32, 512)\n",
            "\n",
            "\t\t\t\t Dec. 4 -> (None, 64, 64, 256)\n",
            "\n",
            "\t\t\t Dec. 3 -> (None, 128, 128, 128)\n",
            "\n",
            "\t\t Dec. 2 -> (None, 256, 256, 64)\n",
            "\n",
            "\t Dec. 1 -> (None, 512, 512, 32)\n",
            "\n",
            "(None, 512, 512, 1)\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 68s 9s/step - loss: 19.8767 - dice_coef: 0.1562 - val_loss: 0.3560 - val_dice_coef: 0.0161\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - ETA: 0s - loss: 0.1438 - dice_coef: 0.0092"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 45s 6s/step - loss: 0.1438 - dice_coef: 0.0092 - val_loss: 0.0988 - val_dice_coef: 0.0345\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0782 - dice_coef: 0.0076"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 56s 7s/step - loss: 0.0782 - dice_coef: 0.0076 - val_loss: 0.0818 - val_dice_coef: 0.0065\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 56s 7s/step - loss: 0.0385 - dice_coef: 0.0068 - val_loss: 0.0415 - val_dice_coef: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0450 - dice_coef: 0.0097 - val_loss: 0.0221 - val_dice_coef: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.0920 - dice_coef: 0.0070"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r8/8 [==============================] - 55s 7s/step - loss: 0.0920 - dice_coef: 0.0070 - val_loss: 0.0024 - val_dice_coef: 0.0330\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 56s 7s/step - loss: 0.0674 - dice_coef: 0.0045 - val_loss: 0.0420 - val_dice_coef: 0.0165\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0038 - dice_coef: 0.0020 - val_loss: 0.0639 - val_dice_coef: 0.0068\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 44s 6s/step - loss: 0.0566 - dice_coef: 0.0097 - val_loss: 0.0396 - val_dice_coef: 0.0070\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 45s 6s/step - loss: 0.0433 - dice_coef: 0.0221 - val_loss: 0.0304 - val_dice_coef: 0.0111\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 56s 7s/step - loss: 0.0913 - dice_coef: 0.0334 - val_loss: 0.0699 - val_dice_coef: 0.0165\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 46s 6s/step - loss: 0.0074 - dice_coef: 0.0101 - val_loss: 0.0693 - val_dice_coef: 0.0248\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 47s 6s/step - loss: 0.0908 - dice_coef: 0.0361 - val_loss: 0.0786 - val_dice_coef: 0.0408\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 56s 7s/step - loss: 0.0275 - dice_coef: 0.0098 - val_loss: 0.0753 - val_dice_coef: 0.0463\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 46s 6s/step - loss: 0.0404 - dice_coef: 0.0205 - val_loss: 0.0595 - val_dice_coef: 0.0318\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0048 - dice_coef: 0.0935 - val_loss: 0.0600 - val_dice_coef: 0.0652\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0358 - dice_coef: 0.1283 - val_loss: 0.0310 - val_dice_coef: 0.0197\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0216 - dice_coef: 0.0132 - val_loss: 0.0617 - val_dice_coef: 0.0302\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 46s 6s/step - loss: 0.0465 - dice_coef: 0.0188 - val_loss: 0.0424 - val_dice_coef: 0.0195\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 55s 7s/step - loss: 0.0143 - dice_coef: 0.0205 - val_loss: 0.0442 - val_dice_coef: 0.0358\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 47s 6s/step - loss: 0.0568 - dice_coef: 0.0680 - val_loss: 0.0491 - val_dice_coef: 0.0453\n",
            "Epoch 22/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('BCE Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val.'], loc='upper right')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['dice_coef'], color='r')\n",
        "plt.plot(history.history['val_dice_coef'])\n",
        "plt.ylabel('Dice Score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EmW4XOxprSMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PweOO63zr-p5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}